{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Chinmay Bharambe \n",
    "- Anshul Govindu \n",
    "- Chaela Moraleja \n",
    "- Candice Sanchez \n",
    "- Praveen Sharma\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using UCSD enrollment data since Fall 2022, what combination of course characteristics (fill rate, capacity, time offered) and student factors (class standing, major) best predict enrollment success rates for undergraduate courses, across all departments, during first and second pass registrations? \n",
    "Can these predictions be used to develop a recommendation tool that optimizes first and second-pass course selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project attempts to address a major challenge for UCSD students: deciding which classes to enroll in during first and second pass. UCSD’s unique “pass” enrollment system turns course selection into more of an art than a science, often leaving students uncertain about their choices or failing to enroll in certain classes. This process also involves other unusual factors, such as major priority for CSE courses. Overall, there is a definite need for a tool that maximizes students' chances of securing their desired courses.\n",
    "\n",
    "Upon initial research, we came across a project that collects data on individual classes at different points in time during each term, such as Fall 2022 or Winter 2023; each term’s data is contained within its own repository <a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1). The project involved building a web scraping tool that scrapes web-reg about every 10 minutes, and collects real-time data on information like enrolled, available, and waitlist spots. This not only offers a tool to collect our own data in the future, but also a great sample dataset from what has already been collected.\n",
    "\n",
    "We also found another project that was built using the aforementioned github repositories <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2). Given a course, the website takes data from specific terms and plots the course availability as a time series across various registration milestones (senior first pass, junior second pass, etc). This offers a great initial visualization of the enrollment data, and our EDA would likely produce some similar graphs. However, we certainly have to build upon this with predictive analyses in order to answer our research question.\n",
    "\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) https://github.com/UCSD-Historical-Enrollment-Data\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) https://www.ucsdregistration.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict that the fill rate of a course and the student’s major would be the most influential combination of factors for students deciding which courses to enroll in during first and second passes. Specifically, we predict that a high course fill rate and close relationship between the course and a student’s major would make it more likely to be enrolled in during first pass rather than second pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "- Dataset #1\n",
    "  - Dataset Name: UCSD Historical Enrollment Data\n",
    "  - Link to the dataset: https://github.com/UCSD-Historical-Enrollment-Data/UCSDHistEnrollData?tab=readme-ov-file\n",
    "  - Number of observations: 11 quarters of data is recorded, the number of observations for subjects across the quarters is inconsistent.\n",
    "  - Number of variables: There are 5 variables recorded: \n",
    "    - Time : The date and time the data was recorded\n",
    "    - Enrolled : Number of students enrolled\n",
    "    - Available : Number of seats available\n",
    "    - Waitlisted : number of students waitlisted\n",
    "    - Total : total seats available for the course \n",
    "\n",
    "This dataset was compiled using an automated web scraper that collected enrollment information from UC San Diego courses, spanning from Fall 2022 through the current quarter. The data is stored in CSV files that are hosted on GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCSD Historical Enrollment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, collecting data from GitHub seemed straightforward since pd.read_csv() can read web links. However, given that the datasets spanned thousands of subjects, each with many thousands of observations across 11 quarters, using pd.read_csv() alone proved inefficient, with an estimated load time of over 12 hours.\n",
    "\n",
    "To address this, we first implemented the chunking mechanism in pd.read_csv() to read the data in smaller segments. While this improved speed, the gain was not substantial. Additionally, we encountered GitHub API rate limits. To overcome this, we used the concurrent.futures package to parallelize the data retrieval process, enabling us to read multiple files simultaneously, significantly improving efficiency<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1). \n",
    "\n",
    "To further mitigate API limitations, we discovered that while unauthenticated users were restricted to 60 requests per hour, authenticated users had a much higher limit of 5,000 requests per hour. By adding authentication headers to our requests, we avoided unnecessary restrictions.\n",
    "\n",
    "Another challenge arose when we realized our API links were fetching directory contents containing CSV files for each quarter. However, GitHub truncates directory listings at 999 files, meaning we were missing additional CSV files. After researching, we found that using the git/trees API allowed us to access all files within a directory, including those hidden by truncation. Implementing this solution ensured that we retrieved the complete dataset<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2).\n",
    "\n",
    "Finally, to prevent the need to rerun this entire process every time the notebook's kernel restarts, we saved the dataset as enrollment_data.csv. If this file exists in the current working directory, the data-loading process is skipped, thereby optimizing efficiency.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) https://medium.com/@smrati.katiyar/introduction-to-concurrent-futures-in-python-009fe1d4592c\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) https://docs.github.com/en/rest/repos/contents?apiVersion=2022-11-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the repo-links that host the data for each quarter in a csv file\n",
    "repo_links = [\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2022Fall/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2023Winter/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2023Spring/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2023Fall/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Winter/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Spring/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Summer1/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Summer2/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Summer3/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2024Fall/contents/overall',\n",
    "    'https://api.github.com/repos/UCSD-Historical-Enrollment-Data/2025Winter/contents/overall',\n",
    "      ]\n",
    "\n",
    "quarter_names = ['FA 22','WI 23', 'SP 23', 'FA 23', 'WI 24', 'SP 24', 'S1 24', 'S2 24', 'S3 24', 'FA 24', 'WI 25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github token for adressing the limit on github api rates. \n",
    "# Recommended to create an environment varaible to store this for improved security. Alternatively, one can simply add the github token below\n",
    "GITHUB_TOKEN = 'put in your token'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if the enrollment data file already exists\n",
    "if os.path.exists('enrollment_data.csv'):\n",
    "    df = pd.concat([\n",
    "    pd.read_csv(f'enrollment_data_part{i+1}.csv') \n",
    "    for i in range(4)], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    # this function processes the data quarter by quarter\n",
    "    def process_quarter(repo_link, quarter_name):\n",
    "        # try-except block to handle errors\n",
    "        try:\n",
    "            # extract repo name from the API URL\n",
    "            repo_name = repo_link.split('/')[5]  \n",
    "            \n",
    "            # construct tree API URL to gett all the files that are hidden as well\n",
    "            tree_url = f\"https://api.github.com/repos/UCSD-Historical-Enrollment-Data/{repo_name}/git/trees/main?recursive=1\"\n",
    "            \n",
    "            # add headers to account for GitHub API rate limiting\n",
    "            headers = {\n",
    "                'Accept': 'application/vnd.github.v3+json',\n",
    "                'Authorization': f'token {GITHUB_TOKEN}'\n",
    "            }\n",
    "            \n",
    "            # get the tree\n",
    "            response = requests.get(tree_url, headers=headers)\n",
    "            \n",
    "            # if request was unsuccessful print error message\n",
    "            if response.status_code != 200:\n",
    "                print(f\"failed to access {tree_url}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "            # get all files from the 'overall' directory\n",
    "            all_files = [item['path'].split('/')[-1] \n",
    "                        for item in response.json()['tree'] \n",
    "                        if item['path'].startswith('overall/') and item['path'].endswith('.csv')]\n",
    "                        \n",
    "            # process multiple files in parallel to make the process faster and more efficient\n",
    "            dfs = []\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                # create a list of futures where each future represents a file being processed\n",
    "                futures = [executor.submit(process_file, file, repo_name, quarter_name) \n",
    "                        for file in all_files]\n",
    "                \n",
    "                # loop through completed futures and collect results\n",
    "                for future in futures:  \n",
    "                    try:\n",
    "                        df = future.result()\n",
    "                        if df is not None:\n",
    "                            dfs.append(df)\n",
    "                    except Exception as e:\n",
    "                        # print error in case of an error\n",
    "                        print(f\"error in future: {str(e)}\")\n",
    "            \n",
    "            # if dfs is not empty, concatenate all the dfs for that quarter and return that\n",
    "            if dfs:\n",
    "                return pd.concat(dfs, ignore_index=True)\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            # print error msg if it occurs\n",
    "            print(f\"error processing quarter {quarter_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # function that reads the csv file and makes it into a df\n",
    "    def process_file(file, repo_name, quarter_name):\n",
    "        print(file)\n",
    "        # try-except block to handle errors\n",
    "        try:\n",
    "            # convert file name to the format seen in the url\n",
    "            file_url = file.replace(' ','%20')\n",
    "            \n",
    "            # raw csv file link\n",
    "            raw_url = f\"https://raw.githubusercontent.com/UCSD-Historical-Enrollment-Data/{repo_name}/main/overall/{file_url}\"\n",
    "            \n",
    "            # add authentication headers\n",
    "            headers = {\n",
    "                'Accept': 'application/vnd.github.v3+json',\n",
    "                'Authorization': f'token {GITHUB_TOKEN}'\n",
    "            }\n",
    "            \n",
    "            # Read the csv files with authentication\n",
    "            response = requests.get(raw_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # read csv file into a pandas df\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(response.text),\n",
    "                sep=',',              # the separator\n",
    "                encoding='utf-8',     # specify the character encoding\n",
    "                parse_dates=['time'], # parse dates as datetime objects as they are being read to save time\n",
    "                usecols=['time', 'enrolled', 'available', 'waitlisted', 'total'] # specify column names to improve efficiency\n",
    "            )\n",
    "            \n",
    "            if not df.empty:\n",
    "                # add course column that is readable\n",
    "                df['course'] = file.replace('.csv', '').replace('%20',' ')\n",
    "                # group df at a frequency of every 12 hrs to get 2 readings for each day\n",
    "                df = df.groupby(pd.Grouper(key='time', freq='12h')).first().reset_index()\n",
    "                # add a column that stores the quarter name\n",
    "                df['quarter'] = quarter_name\n",
    "                return df\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            # if there is an error, print it\n",
    "            print(f\"error processing {file}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def load_data():\n",
    "        # list that will store the df for every quarter\n",
    "        all_quarter_dfs = []\n",
    "\n",
    "        # loop through each quarter and process its data\n",
    "        for repo_link, quarter_name in zip(repo_links, quarter_names):  \n",
    "            # delay to avoid hitting githubs rate limits\n",
    "            if all_quarter_dfs:\n",
    "                time.sleep(5)\n",
    "\n",
    "            # process the current quarters data   \n",
    "            quarter_df = process_quarter(repo_link, quarter_name)\n",
    "\n",
    "            # append data to all_quarter_dfs if df is not empty\n",
    "            if quarter_df is not None:\n",
    "                all_quarter_dfs.append(quarter_df)\n",
    "                \n",
    "                # save progress after each quarter in case the program crashes            \n",
    "                temp_df = pd.concat(all_quarter_dfs, ignore_index=True)\n",
    "                temp_df.to_csv('enrollment_data_temp.csv', \n",
    "                            index=False,\n",
    "                            encoding='utf-8')\n",
    "        \n",
    "        # save the final complete dataset\n",
    "        if all_quarter_dfs:\n",
    "            combined_df = pd.concat(all_quarter_dfs, ignore_index=True)\n",
    "            combined_df.to_csv('enrollment_data.csv', \n",
    "                            index=False,\n",
    "                            encoding='utf-8')\n",
    "            return combined_df\n",
    "        return None\n",
    "\n",
    "    # Run the load_data function\n",
    "    df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the data from github we did not realise that there were some graduate level courses included in this datasets. Therefore, we will exclude these courses below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>enrolled</th>\n",
       "      <th>available</th>\n",
       "      <th>waitlisted</th>\n",
       "      <th>total</th>\n",
       "      <th>course</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-18 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>AAS 10</td>\n",
       "      <td>FA 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-18 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>AAS 10</td>\n",
       "      <td>FA 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-19 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>AAS 10</td>\n",
       "      <td>FA 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-19 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>AAS 10</td>\n",
       "      <td>FA 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-20 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>AAS 10</td>\n",
       "      <td>FA 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069658</th>\n",
       "      <td>2025-01-25 00:00:00</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>WCWP 10B</td>\n",
       "      <td>WI 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069659</th>\n",
       "      <td>2025-01-25 12:00:00</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>WCWP 10B</td>\n",
       "      <td>WI 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069660</th>\n",
       "      <td>2025-01-26 00:00:00</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>WCWP 10B</td>\n",
       "      <td>WI 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069661</th>\n",
       "      <td>2025-01-31 00:00:00</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>WCWP 10B</td>\n",
       "      <td>WI 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069662</th>\n",
       "      <td>2025-01-31 12:00:00</td>\n",
       "      <td>319.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>WCWP 10B</td>\n",
       "      <td>WI 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2278752 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  enrolled  available  waitlisted  total    course  \\\n",
       "0       2022-05-18 00:00:00       0.0       68.0         0.0   68.0    AAS 10   \n",
       "1       2022-05-18 12:00:00       0.0       68.0         0.0   68.0    AAS 10   \n",
       "2       2022-05-19 00:00:00       0.0       68.0         0.0   68.0    AAS 10   \n",
       "3       2022-05-19 12:00:00       0.0       68.0         0.0   68.0    AAS 10   \n",
       "4       2022-05-20 00:00:00       0.0       68.0         0.0   68.0    AAS 10   \n",
       "...                     ...       ...        ...         ...    ...       ...   \n",
       "3069658 2025-01-25 00:00:00     319.0        1.0        12.0  320.0  WCWP 10B   \n",
       "3069659 2025-01-25 12:00:00     319.0        1.0        12.0  320.0  WCWP 10B   \n",
       "3069660 2025-01-26 00:00:00     319.0        1.0        12.0  320.0  WCWP 10B   \n",
       "3069661 2025-01-31 00:00:00     319.0        1.0        12.0  320.0  WCWP 10B   \n",
       "3069662 2025-01-31 12:00:00     319.0        1.0        12.0  320.0  WCWP 10B   \n",
       "\n",
       "        quarter  \n",
       "0         FA 22  \n",
       "1         FA 22  \n",
       "2         FA 22  \n",
       "3         FA 22  \n",
       "4         FA 22  \n",
       "...         ...  \n",
       "3069658   WI 25  \n",
       "3069659   WI 25  \n",
       "3069660   WI 25  \n",
       "3069661   WI 25  \n",
       "3069662   WI 25  \n",
       "\n",
       "[2278752 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the course number and convert to integer\n",
    "df['course_number'] = df['course'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Filter out graduate courses where the number is >=200\n",
    "df = df[df['course_number'] < 200]\n",
    "\n",
    "# Drop the temporary course_number column \n",
    "df = df.drop('course_number', axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "- How will you handle issues you identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 108 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object_detection_env",
   "language": "python",
   "name": "object_detection_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
